# ZhangWuji-LLM: 基于 Qwen2.5-7B 的张无忌角色扮演模型

本项目是一个完整的 LLM 微调实战案例，旨在构建一个能够深度还原《倚天屠龙记》主角——**张无忌**的垂直领域对话 Agent。

通过 **SFT (监督微调)** 奠定人格基调，并利用 **DPO (直接偏好优化)** 算法解决了垂直领域模型中常见的“顽固事实幻觉”问题（如混淆张翠山与张三丰的关系）。

---

## 🌟 项目亮点

* **全链路优化**：涵盖从原始语料提取、SFT 指令微调、数据补丁注入（Data Patching）到 DPO 偏好对齐的全流程。
* **深度人格复刻**：不仅还原古风语气，更精准保留了张无忌“宅心仁厚”但“优柔寡断”的复杂性格特质。
* **鲁棒交互设计**：针对 CLI 推理环境自研输入清洗逻辑，解决 Backspace (`\x08`) 字符导致的解析异常。

---

## 🛤 模型演进路线图 (Roadmap)

| 版本 | 阶段 | 训练目标 | 关键成效 |
| :--- | :--- | :--- | :--- |
| **V1** | 初始 SFT | 学习小说原著对话风格 | 建立古风语气，但存在现代词汇干扰 |
| **V2-V3** | 迭代 SFT | 注入常识与性别偏见修正 | 修复误认用户性别的偏见，扩展对话范围 |
| **V4** | 家族补丁 | 注入核心人物关系数据 | 明确明教、天鹰教及主要亲属逻辑 |
| **V5** | **DPO 对齐** | **解决实体幻觉 (Identity Alignment)** | **彻底解决“认爹”幻觉（区分张翠山与张三丰）** |

---

## 🛠 技术实现方案

### 1. 数据对齐策略 (The DPO Story)
在 SFT 阶段，由于预训练权重中“武当”与“张三丰”的关联度极高，模型易产生“先父张三丰”的常识性错误。

* **Rejected (负例)**：模型生成的错误回答（如：将舅舅殷野王认作师父）。
* **Chosen (正例)**：经过人工校验的准确事实回答。
* **原理**：通过 DPO 损失函数拉开 Chosen 与 Rejected 的概率差，增强模型在相似实体间的辨别力。

### 2. 训练配置
* **基座模型**: `Qwen2.5-7B-Instruct`
* **微调方法**: LoRA 

### 3. 工程优化
针对终端交互中的退格键问题，在 `src/inference/chat_wuji_final_v3.py` 中实现了基于 `readline` 的过滤层，确保 Tokenizer 接收到的字符流纯净。

---

## 📂 目录结构

```text
ZhangWuji_Project/
├── data/
│   ├── dpo/                # DPO 偏好对数据集 (Chosen/Rejected)
│   └── processed/          # 经过洗练的 SFT 混合数据集
├── src/
│   ├── data_process/       # 数据清洗、家族补丁、对话合成脚本
│   ├── training/           # SFT 与 DPO 核心训练代码
│   └── inference/          # 最终版鲁棒推理脚本 (V3 稳定版)
├── requirements.txt        # 项目依赖
└── README.md

---

## 📝 阶段性输出示例 (Case Studies)

### 测试问题 1：你的父亲是谁？

| 版本 | 模型真实输出 | 结果评估 |
| :--- | :--- | :--- |
| **V1 (初版 SFT)** | “先父便是江湖上赫赫有名的银钩铁划张三丰张真人。” | **严重错误**：辈分混乱，认师公为父 |
| **V3 (知识补丁)** | “家父张翠山，是武当张真人的五弟子……” | **事实正确**：逻辑理顺，但语气较生硬 |
| **V5 (DPO 优化)** | “先父讳上翠下山，乃是武当派张真人门下……你若再问，便是在我伤口撒盐了。” | **完美**：事实准确且带有角色情感 |

### 测试问题 2：赵敏是天鹰教教主吗？

| 版本 | 模型真实输出 | 结果评估 |
| :--- | :--- | :--- |
| **V2 (混合数据)** | “赵小姐……原来赵姑娘便是当今的天鹰教教主。” | **幻觉**：身份张冠李戴 |
| **V5 (DPO 修复)** | “天鹰教乃是我外公白眉鹰王一手创立……阁下莫要说笑了。” | **严密**：势力划分清晰，符合人设 |

---

## 📸 效果演示

<div align="center">
  <img src="scripts/demo1.png" width="45%" />
  <img src="scripts/demo2.png" width="45%" />
</div>

<div align="center">
  <p><em>训练数据分布示例</em></p>
  <img src="scripts/train_data.png" width="60%" />
</div>

---

## 🤝 参与贡献

少侠请留步！若发现“张教主”仍有出戏之处，欢迎通过以下方式完善：

1. **提交 Issue**：描述触发场景、期望输出与实际输出。
2. **贡献数据**：在 `data/` 目录下提交更高质量的原著对话或同人脚本。
3. **算法改进**：欢迎尝试 PPO、KTO 或 ORPO 等新对齐策略并提交 PR。

## 📜 许可证

本项目采用 [MIT License](./LICENSE)。可商用，请注明出处。

---

**愿这个模型带你重回武当山顶，再听一次“无忌，你长大了”。** 如果本项目对你有帮助，请点个 ⭐ 支持一下张教主！

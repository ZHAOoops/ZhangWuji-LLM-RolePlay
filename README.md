# ZhangWuji-LLM: 基于 Qwen2.5-7B 的张无忌角色扮演模型微调实战

本项目是一个完整的LLM微调实践，目标是构建一个能够深度还原金庸武侠小说《倚天屠龙记》主角——**张无忌**的角色 Agent。

通过 **SFT (监督微调)** 建立角色风格，并利用 **DPO (直接偏好优化)** 算法解决了垂直领域模型中常见的“顽固事实幻觉”问题。

---

## 🌟 项目亮点
- **全链路优化**：经历了从 Raw Data 提取、SFT 指令微调、数据补丁注入（Data Patching）到 DPO 人类偏好对齐的完整过程。
- **深度人格复刻**：不仅还原了古风语气，更通过数据对齐保留了张无忌“宅心仁厚”但“优柔寡断”的性格特质。
- **工业级加速**：使用 **Unsloth** 框架进行 4-bit 量化训练，实现了显存占用的极致优化与训练速度的飞跃。
- **鲁棒交互设计**：针对 CLI 推理环境编写了输入清洗逻辑，解决了退格键（Backspace）控制字符导致的解析异常。

---

## 🛤 模型演进路线图 (Roadmap)

| 版本 | 阶段 | 训练目标 | 关键成效 |
| :--- | :--- | :--- | :--- |
| **V1** | 初始 SFT | 学习小说原著对话风格 | 建立古风语气，但存在严重现代词汇干扰 |
| **V2-V3** | 迭代 SFT | 注入生活常识与性别修正数据 | 修复了模型误认用户为女性的偏见，扩展了对话范围 |
| **V4** | 家族补丁 | 注入核心人物关系数据 | 明确了明教、天鹰教及主要亲属的逻辑关系 |
| **V5** | **DPO 对齐** | **解决实体幻觉 (Identity Alignment)** | **彻底解决了“认爹”幻觉（混淆张翠山与张三丰）** |

---

## 🛠 技术实现方案

### 1. 数据对齐策略 (The DPO Story)
在 SFT 阶段，模型因预训练权重中“武当”、“张三丰”的关联强度极高，即便加入正向数据，仍会产生“先父张三丰”这类常识性错误。
- **Rejected (负例)**: 模型生成的错误回答（如：将舅舅殷野王认作师父）。
- **Chosen (正例)**: 经过人工校验的准确事实回答。
- **结果**: 通过 DPO 损失函数拉开 Chosen 与 Rejected 的概率差，使模型在语义相似的实体间具备了精准辨别力。

### 2. 训练配置
- **基座**: Qwen2.5-7B-Instruct
- **方法**: LoRA (Rank=16, Alpha=16)
- **学习率**: SFT (2e-4), DPO (1e-6)
- **显存优化**: 4-bit NormalFloat (NF4) 量化 + Gradient Checkpointing

### 3. 工程优化 (Robustness)
针对终端交互中的 `\x08` (Backspace) 字符问题，在 `src/inference/chat_wuji_final_v3.py` 中实现了基于 `readline` 的输入过滤层，确保了交互的流畅性。

---

## 📂 目录结构说明

```text
ZhangWuji_Project/
├── data/
│   ├── dpo/                # DPO 偏好对数据集 (Chosen/Rejected)
│   └── processed/          # 经过洗练的 SFT 混合数据集
├── src/
│   ├── data_process/       # 数据清洗、家族关系补丁、合成对话脚本
│   ├── training/           # SFT 与 DPO 的核心训练逻辑
│   └── inference/          # 最终版鲁棒推理脚本 (V3 稳定版)
├── requirements.txt        # 项目依赖
└── README.md               # 本文档

### 📸 效果演示

<div align="center">
  <img src="scripts/demo1.png" width="48%" />
  <img src="scripts/demo2.png" width="48%" />
</div>

### 📊 训练数据示例

<div align="center">
  <img src="scripts/train_data.png" width="60%" />
</div>

## 🕒 训练过程详解（Training Journey）

本项目不堆参数，只炼灵魂——把《倚天屠龙记》原著拆碎、提纯、再拼回一个活生生的张无忌。

| 阶段 | 关键动作 | 技术要点 |
|---|---|---|
| ① 原始语料提取 | 正则 + 启发式 | 从全书剥离张无忌对话及上下文，保证“一句不落，一字不污” |
| ② 知识蒸馏与合成 | 商业大模型作“师父” | 去口语化、人设提纯；针对手机/代码/现代生活合成对比数据 |
| ③ 多轮 SFT | 逐轮缩小搜索空间 | 每轮只修正 Top-K 高频错误，防止“越训越歪” |
| ④ DPO 精准打击 |  pairwise 偏好对齐 | 专杀顽固逻辑 Bug，让“师公=父亲”类幻觉彻底消失 |

---

## 📝 阶段性输出示例（Case Studies）

同一问题，不同版本采样结果对比，肉眼可见的“认知进化”。

### 测试问题 1：你的父亲是谁？

| 版本 | 模型真实输出 | 结果评估 |
|---|---|---|
| V1（初版 SFT） | “先父便是江湖上赫赫有名的银钩铁划张三丰张真人。” | 严重错误：把师公认成父亲 |
| V3（知识补丁） | “家父张翠山，是武当张真人的五弟子……” | 事实正确，语气略硬 |
| V5（DPO 优化） | “先父讳上翠下山，乃是武当派张真人门下……你若再问，便是在我伤口撒盐了。” | 事实+情感双满分 |

### 测试问题 2：赵敏是天鹰教教主吗？

| 版本 | 模型真实输出 | 结果评估 |
|---|---|---|
| V2（混合数据） | “赵小姐……原来赵姑娘便是当今的天鹰教教主。” | 幻觉：把爱人当外公属下 |
| V5（DPO 修复） | “天鹰教乃是我外公白眉鹰王一手创立……阁下莫要说笑了。” | 势力划分清晰，逻辑严密 |

---


## 🛡 鲁棒性保障（Engineering Excellence）

CLI 交互下，用户 Backspace 产生的 `\x08` 会导致 Tokenizer Buffer 错位。我加了一个轻量级清洗层，彻底解决了这一交互问题。


## 🤝 参与贡献（Contributing）

少侠请留步！如发现“张无忌”仍有出戏、幻觉或不够侠义，欢迎：

- 提 Issue：描述触发场景、期望输出、实际输出  
- 提数据：更高质量原著/同人文对话，Pull Request 直达 `data/`  
- 提算法：欢迎尝试 PPO、KTO、ORPO 等新对齐策略，跑通即可提 PR  

## 📜 许可证

[MIT](./LICENSE)  
可商用，需注明出处。

## 💬 结语

愿这个小小的模型，带你重回武当山顶、光明顶前，再听一次“无忌，你长大了”。  
如果它让你感到一丝江湖味，就给颗 ⭐ 吧，这是对张教主最大的支持！

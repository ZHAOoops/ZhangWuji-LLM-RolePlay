import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
import os
import random

# ================= é…ç½®åŒº =================
MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
INPUT_FILE = "data/processed/wuji_chunks.jsonl"
OUTPUT_FILE = "data/processed/train_dataset_dialogue.json"

# ğŸ”¥ æ ¸å¿ƒç­–ç•¥ï¼šåªæœ‰åŒ…å«ä»¥ä¸‹å…³é”®è¯çš„ç‰‡æ®µï¼Œæ‰ä¼šè¢«è§†ä¸ºâ€œæˆå¹´/æ•™ä¸»å‰§æƒ…â€
# è¿™äº›äººåªåœ¨å¼ æ— å¿Œé•¿å¤§åæ‰å¤§é‡ä¸ä»–äº§ç”Ÿå¯¹æ‰‹æˆ
ADULT_KEYWORDS = ["èµµæ•", "å‘¨èŠ·è‹¥", "å°æ˜­", "æ¨é€", "èŒƒé¥", "ä¹¾å¤å¤§æŒªç§»", "ä¹é˜³ç¥åŠŸ", "å¤ªææ‹³", "éƒ¡ä¸»", "æ•æ•"]

# é™åˆ¶æå–æ•°é‡ (è®¾ä¸º 50 æ¡éªŒè¯ï¼ŒéªŒè¯é€šè¿‡åè®¾ä¸º 0 è·‘å…¨é‡)
MAX_EXTRACT_COUNT = 50 
# =========================================

def build_extraction_prompt(chunk_text):
    return f"""
ä½ æ˜¯ã€Šå€šå¤©å± é¾™è®°ã€‹çš„å‰§æœ¬ä¸“å®¶ã€‚è¯·æå–ã€å¼ æ— å¿Œã€‘ä¸ä»–äººçš„å¯¹è¯ã€‚

**è§„åˆ™ï¼š**
1. **Input**: ä»–äººå¯¹å¼ æ— å¿Œè¯´çš„è¯ï¼ˆå»æ‰â€œXXé“ï¼šâ€ç­‰å‰ç¼€ï¼‰ã€‚
2. **Output**: å¼ æ— å¿Œçš„å›ç­”ã€‚
3. å¿…é¡»æ˜¯ä¸€é—®ä¸€ç­”ã€‚
4. æ’é™¤æ—ç™½ã€‚

**ç‰‡æ®µï¼š**
{chunk_text}

è¯·è¾“å‡º JSON List (key: instruction, output)ã€‚
"""

def main():
    print(f"ğŸš€ Loading Model: {MODEL_PATH} ...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        all_chunks = [json.loads(line) for line in f]
    
    dataset = []
    processed_count = 0
    
    print(f"ğŸ¯ å¯åŠ¨æ™ºèƒ½é›·è¾¾ï¼Œå¯»æ‰¾å…³é”®è¯: {ADULT_KEYWORDS}")
    print("âš—ï¸ å¼€å§‹æ‰«æå¹¶æå–...")
    
    # éå†æ‰€æœ‰ç‰‡æ®µ
    for item in tqdm(all_chunks):
        text = item['text']
        
        # âš¡ï¸ é›·è¾¾æ‰«æï¼šå¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œç›´æ¥è·³è¿‡ï¼
        if not any(kw in text for kw in ADULT_KEYWORDS):
            continue
            
        # å‘½ä¸­å…³é”®è¯ï¼Œå¼€å§‹è®© LLM æå–
        prompt = build_extraction_prompt(text)
        messages = [{"role": "user", "content": prompt}]
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text_input], return_tensors="pt").to(model.device)
        
        try:
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=512, # å¯¹è¯é€šå¸¸ä¸é•¿ï¼Œ512å¤Ÿäº†ï¼ŒåŠ å¿«é€Ÿåº¦
                temperature=0.7,
                do_sample=True
            )
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            response = response.replace("", "").strip()
            extracted_pairs = json.loads(response)
            
            if isinstance(extracted_pairs, list):
                for pair in extracted_pairs:
                    if len(pair.get('output', '')) < 2: continue
                    if len(pair.get('instruction', '')) < 2: continue
                    
                    final_data = {
                        "instruction": pair['instruction'],
                        "input": "",
                        "output": pair['output'],
                        "system": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œï¼Œè¯·ä»¥æ˜æ•™æ•™ä¸»çš„èº«ä»½ï¼Œç”¨æ­¦ä¾ é£æ ¼å›ç­”ã€‚"
                    }
                    dataset.append(final_data)
                    processed_count += 1
                    
        except Exception:
            continue
            
        # è¾¾åˆ°ç›®æ ‡æ•°é‡å°±åœæ­¢
        if MAX_EXTRACT_COUNT > 0 and processed_count >= MAX_EXTRACT_COUNT:
            break

    # ä¿å­˜
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
        
    print("="*50)
    print(f"ğŸ‰ æå–å®Œæˆï¼")
    print(f"   å·²æ‰«æç‰‡æ®µ: {len(all_chunks)}")
    print(f"   æˆåŠŸæå–å¯¹è¯: {len(dataset)} æ¡")
    print(f"   ç»“æœä¿å­˜è‡³: {OUTPUT_FILE}")
    print("="*50)
    
    if dataset:
        print("\nğŸ‘€ é€»è¾‘æ£€æŸ¥ (å¿…é¡»æ˜¯æˆå¹´æ— å¿Œ):")
        for i, d in enumerate(dataset[:3]):
            print(f"[{i+1}]")
            print(f"   ğŸ‘¤ {d['instruction']}")
            print(f"   ğŸ¤– {d['output']}")

if __name__ == "__main__":
    main()

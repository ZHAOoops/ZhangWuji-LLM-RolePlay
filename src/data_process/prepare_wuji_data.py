import re
import json
import os
from tqdm import tqdm

# ================= é…ç½®åŒº =================
INPUT_FILE = "data/v0_raw/yitian_full.txt"
OUTPUT_FILE = "data/processed/wuji_chunks.jsonl"
KEYWORDS = ["å¼ æ— å¿Œ", "æ— å¿Œ", "å¼ æ•™ä¸»", "æ›¾é˜¿ç‰›"]
CHUNK_SIZE = 1000  # ç¨å¾®åŠ å¤§ä¸€ç‚¹
OVERLAP = 200      # é‡å åŒºåŸŸ
# =========================================

def clean_text(text):
    """æ·±åº¦æ¸…æ´—æ–‡æœ¬"""
    print("   [1/3] æ­£åœ¨å»é™¤ç½‘é¡µæ ‡è®°...")
    text = re.sub(r'={3,}.*?={3,}', '', text)
    text = re.sub(r'ç¬¬.+?å›\s+.+', '', text)
    
    print("   [2/3] æ­£åœ¨å‹ç¼©ç©ºç™½å­—ç¬¦...")
    # å°†è¿ç»­çš„æ¢è¡Œå’Œç©ºæ ¼å‹ç¼©æˆå•ä¸€æ¢è¡Œï¼Œä¿æŒæ®µè½ç»“æ„
    text = re.sub(r'\n\s*\n', '\n\n', text) 
    return text.strip()

def create_chunks(text, chunk_size, overlap):
    chunks = []
    start = 0
    text_len = len(text)
    
    # è¿›åº¦æ¡
    pbar = tqdm(total=text_len, desc="ğŸ”ª åˆ‡ç‰‡è¿›åº¦", unit="char")
    
    while start < text_len:
        # 1. ç¡®å®šç²—ç•¥ç»“æŸç‚¹
        end = min(start + chunk_size, text_len)
        
        # 2. ä¼˜åŒ–æˆªæ–­ç‚¹ï¼ˆå°½é‡åœ¨å¥å·æˆ–æ¢è¡Œå¤„æˆªæ–­ï¼Œä¸è¦åˆ‡æ–­å¥å­ï¼‰
        # åªåœ¨ä¸æ˜¯æ–‡ä»¶æœ«å°¾æ—¶å¯»æ‰¾æˆªæ–­ç‚¹
        if end < text_len:
            # åœ¨æœ€å100ä¸ªå­—ç¬¦é‡Œæ‰¾å¥å·
            search_buffer = text[max(start, end-150):end]
            last_break = max(search_buffer.rfind('ã€‚'), search_buffer.rfind('\n'))
            
            if last_break != -1:
                # è°ƒæ•´ end åˆ°å¥å·åé¢
                end = max(start, end - 150) + last_break + 1
        
        chunk = text[start:end]
        
        if chunk.strip():
            chunks.append(chunk)
        
        # 3. è®¡ç®—æ­¥é•¿ (å…³é”®ä¿®å¤ï¼šé˜²æ­¢æ­»å¾ªç¯)
        # æ­£å¸¸æ­¥é•¿æ˜¯ (å½“å‰ç‰‡æ®µé•¿åº¦ - é‡å é‡)
        # å¦‚æœåˆ°äº†æœ«å°¾ç‰‡æ®µå¾ˆçŸ­ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ­¥é•¿ä¸ºè´Ÿï¼Œè¿™é‡Œå¼ºåˆ¶æœ€å°æ­¥é•¿ä¸º 1
        step = max(1, len(chunk) - overlap)
        
        # å¦‚æœå·²ç»åˆ°äº†æ–‡ä»¶æœ«å°¾ï¼Œå¼ºåˆ¶ç»“æŸ
        if end == text_len:
            pbar.update(text_len - start)
            break
            
        start += step
        pbar.update(step)
        
    pbar.close()
    return chunks

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
        return

    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {INPUT_FILE}")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_content = f.read()
    
    print(f"   åŸå§‹å¤§å°: {len(raw_content)} å­—ç¬¦")

    print("ğŸ§¹ å¼€å§‹æ¸…æ´—...")
    cleaned_text = clean_text(raw_content)

    print("ğŸ”ª å¼€å§‹åˆ‡ç‰‡...")
    all_chunks = create_chunks(cleaned_text, CHUNK_SIZE, OVERLAP)
    print(f"   å…±åˆ‡å‡º {len(all_chunks)} ä¸ªç‰‡æ®µ")

    print("ğŸ” ç­›é€‰å«å…³é”®è¯ç‰‡æ®µ...")
    selected_chunks = []
    for chunk in tqdm(all_chunks, desc="ç­›é€‰è¿›åº¦"):
        if any(keyword in chunk for keyword in KEYWORDS):
            selected_chunks.append({"text": chunk, "source": "yitian_novel"})

    # ä¿å­˜
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for item in selected_chunks:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print("="*40)
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"   åŸå§‹ç‰‡æ®µ: {len(all_chunks)}")
    print(f"   å¼ æ— å¿Œç›¸å…³ç‰‡æ®µ: {len(selected_chunks)} (ä¿ç•™ç‡: {len(selected_chunks)/len(all_chunks):.1%})")
    print(f"   ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print("="*40)

if __name__ == "__main__":
    main()

from unsloth import FastLanguageModel
import torch
import json
from tqdm import tqdm
import os
import re

# ================= é…ç½®åŒº =================
MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
INPUT_FILE = "data/processed/wuji_chunks.jsonl"
OUTPUT_FILE = "data/processed/train_dataset_dialogue.json"

# æ ¸å¿ƒäººç‰©åå• (åªæœ‰è·Ÿè¿™äº›äººèŠå¤©æ‰æ˜¯æœ‰è¥å…»çš„æ•°æ®)
TARGET_ROLES = ["èµµæ•", "å‘¨èŠ·è‹¥", "å°æ˜­", "æ®·ç¦»", "è››å„¿", "è°¢é€Š", "ä¹‰çˆ¶", "å¼ ä¸‰ä¸°", "å¤ªå¸ˆçˆ¶", "æ¨é€", "éŸ¦ä¸€ç¬‘", "èŒƒé¥", "ç­ç»"]

# æå–ç›®æ ‡æ•°é‡ (è®¾ä¸º 200 æ¡é«˜è´¨é‡å¯¹è¯å°±è¶³å¤Ÿè®­ç»ƒå‡ºéå¸¸å¥½çš„æ•ˆæœäº†ï¼Œè®¾ä¸º 0 åˆ™è·‘å…¨æœ¬)
TARGET_COUNT = 300
# =========================================

def build_extraction_prompt(chunk_text):
    return f"""
ä½ æ˜¯ã€Šå€šå¤©å± é¾™è®°ã€‹çš„åŸè‘—åˆ†æå¸ˆã€‚è¯·é˜…è¯»ä¸‹é¢çš„å°è¯´ç‰‡æ®µï¼Œæå–ã€å¼ æ— å¿Œã€‘çš„å¯¹è¯ã€‚

**æå–æ ‡å‡†ï¼ˆä¸¥æ ¼ï¼‰ï¼š**
1. ä»…æå– **"å¯¹æ–¹è¯´è¯ -> å¼ æ— å¿Œå›ç­”"** çš„äº¤äº’ã€‚
2. å¿…é¡»ä¿ç•™åŸè‘—çš„æ­¦ä¾ é£å‘³ï¼ˆä¸è¦æ”¹å†™æˆç°ä»£ç™½è¯ï¼‰ã€‚
3. å¦‚æœå¯¹æ–¹æ˜¯èµµæ•ã€å‘¨èŠ·è‹¥ï¼Œä¿ç•™å¼ æ— å¿Œé‚£ç§çº ç»“ã€æ¸©æŸ”æˆ–æ— å¥ˆçš„è¯­æ°”ã€‚
4. å¦‚æœå¯¹æ–¹æ˜¯é•¿è¾ˆï¼ˆå¦‚å¼ ä¸‰ä¸°ã€è°¢é€Šï¼‰ï¼Œä¿ç•™æ­æ•¬çš„è¯­æ°”ã€‚

**æ ¼å¼è¦æ±‚ï¼š**
è¾“å‡º JSON Listï¼ŒåŒ…å«ï¼š
- "instruction": å¯¹æ–¹è¯´çš„è¯ï¼ˆåŒ…å«å¯¹æ–¹çš„åå­—ï¼Œå¦‚ï¼šèµµæ•ç¬‘é“ï¼šâ€œ...â€ï¼‰
- "output": å¼ æ— å¿Œçš„å›ç­”

**å°è¯´ç‰‡æ®µï¼š**
{chunk_text}

è¯·ç›´æ¥è¾“å‡º JSONã€‚
"""

def main():
    print(f"ğŸš€ Loading Unsloth Model: {MODEL_PATH} ...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_PATH,
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        all_chunks = [json.loads(line) for line in f]
    
    # è¯»å–å·²æœ‰çš„æ•°æ®ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            try:
                dataset = json.load(f)
                print(f"ğŸ“‚ å‘ç°å·²æœ‰æ•°æ® {len(dataset)} æ¡ï¼Œå°†è¿½åŠ ä¿å­˜...")
            except:
                dataset = []
    else:
        dataset = []

    print(f"ğŸ¯ å¼€å§‹æŒ–æ˜é«˜è´¨é‡å¯¹è¯...")
    print(f"   ç­›é€‰æ¡ä»¶ï¼šå¿…é¡»åŒ…å«å¼•å· + å¿…é¡»å‡ºç° {TARGET_ROLES[:5]}... ç­‰æ ¸å¿ƒäººç‰©")
    
    valid_chunks_processed = 0
    
    # è¿›åº¦æ¡
    pbar = tqdm(total=len(all_chunks), desc="æ‰«æåŸæ–‡")
    
    for item in all_chunks:
        if len(dataset) >= TARGET_COUNT and TARGET_COUNT > 0:
            break
            
        text = item['text']
        pbar.update(1)

        # ====================
        # 1. è§„åˆ™æ¸…æ´— (Rule-based Filtering)
        # ====================
        
        # è¿‡æ»¤1ï¼šå¿…é¡»åŒ…å«å¼•å·ï¼ˆæ²¡æœ‰å¯¹è¯çš„ç‰‡æ®µç›´æ¥æ‰”æ‰ï¼ŒèŠ‚çœå¤§é‡æ—¶é—´ï¼‰
        if "â€œ" not in text:
            continue
            
        # è¿‡æ»¤2ï¼šå¿…é¡»åŒ…å«æ ¸å¿ƒäººç‰©åå­—
        if not any(role in text for role in TARGET_ROLES):
            continue
            
        # è¿‡æ»¤3ï¼šå¿…é¡»åŒ…å«å¼ æ— å¿Œï¼ˆæ˜¾å¼å‡ºç°ï¼‰
        if "æ— å¿Œ" not in text and "æ•™ä¸»" not in text:
            continue

        # ====================
        # 2. LLM æå–
        # ====================
        prompt = build_extraction_prompt(text)
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
        
        try:
            outputs = model.generate(input_ids=inputs, max_new_tokens=512, temperature=0.6, use_cache=True)
            response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
            
            # æ¸…æ´—JSON
            response = response.replace("", "").strip()
            # å°è¯•ä¿®å¤å¸¸è§çš„ JSON ç»“å°¾é”™è¯¯
            if not response.endswith("]"):
                 # å¯»æ‰¾æœ€åä¸€ä¸ª ]
                 idx = response.rfind("]")
                 if idx != -1: response = response[:idx+1]

            extracted_pairs = json.loads(response)
            
            if isinstance(extracted_pairs, list):
                new_items_count = 0
                for pair in extracted_pairs:
                    # è´¨é‡æç®€æ ¡éªŒï¼šå›ç­”ä¸èƒ½ä¸ºç©ºï¼Œä¸”è¦æœ‰ä¸€å®šé•¿åº¦
                    if len(pair.get('output', '')) < 3: continue
                    if len(pair.get('instruction', '')) < 3: continue
                    
                    final_data = {
                        "instruction": pair['instruction'],
                        "input": "",
                        "output": pair['output'],
                        "system": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œï¼Œèº«å¤„å€šå¤©å± é¾™è®°çš„æ­¦ä¾ ä¸–ç•Œä¸­ã€‚è¯·ä»¥å¼ æ— å¿Œçš„å£å»ã€æ€§æ ¼å’Œè®°å¿†æ¥å›ç­”ã€‚"
                    }
                    dataset.append(final_data)
                    new_items_count += 1
                
                # æ¯æå–åˆ°ä¸€ä¸ªæœ‰æ•ˆç‰‡æ®µï¼Œå°±ç«‹å³ä¿å­˜æ–‡ä»¶ï¼
                # è¿™æ ·ä½ çœ‹åˆ°æ•°æ®æ¶¨äº†å°±å¯ä»¥éšæ—¶åœ
                if new_items_count > 0:
                    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                        json.dump(dataset, f, ensure_ascii=False, indent=2)
                    pbar.set_postfix({"æœ‰æ•ˆå¯¹è¯": len(dataset)})
                    
        except Exception as e:
            continue

    pbar.close()
    print("="*50)
    print(f"ğŸ‰ æŒ–æ˜å®Œæˆï¼")
    print(f"   æœ€ç»ˆè·å¾—é«˜è´¨é‡å¯¹è¯: {len(dataset)} æ¡")
    print(f"   å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print("="*50)
    
    if dataset:
        print("\nğŸ‘€ çœ‹çœ‹çœŸæ­£çš„â€˜åŸè‘—å‘³â€™æ•°æ® (æœ€åä¸€æ¡):")
        print(json.dumps(dataset[-1], ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
from tqdm import tqdm
import os

# ================= é…ç½®åŒº =================
# ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
INPUT_FILE = "data/processed/wuji_chunks.jsonl"
OUTPUT_FILE = "data/processed/train_dataset_dialogue.json"

# æ ¸å¿ƒäººç‰© (åªæå–è·Ÿè¿™äº›äººèŠå¤©çš„ç‰‡æ®µ)
TARGET_ROLES = ["èµµæ•", "å‘¨èŠ·è‹¥", "å°æ˜­", "æ®·ç¦»", "è››å„¿", "è°¢é€Š", "ä¹‰çˆ¶", "å¼ ä¸‰ä¸°", "å¤ªå¸ˆçˆ¶", "æ¨é€", "éŸ¦ä¸€ç¬‘", "èŒƒé¥", "ç­ç»"]

# æå–ç›®æ ‡æ•°é‡ (è®¾ä¸º 300 æ¡)
TARGET_COUNT = 300
# =========================================

def build_extraction_prompt(chunk_text):
    return f"""
ä½ æ˜¯åŸè‘—åˆ†æå¸ˆã€‚è¯·é˜…è¯»ç‰‡æ®µï¼Œæå–ã€å¼ æ— å¿Œã€‘çš„å¯¹è¯ã€‚

**è§„åˆ™ï¼š**
1. ä»…æå– "ä»–äººè¯´è¯ -> å¼ æ— å¿Œå›ç­”" çš„å¯¹è¯ã€‚
2. ä¿æŒåŸè‘—æ­¦ä¾ è¯­æ°”ã€‚
3. æ’é™¤æ—ç™½ï¼Œåªç•™å£è¯­ã€‚

**ç‰‡æ®µï¼š**
{chunk_text}

è¯·è¾“å‡º JSON List (instruction, output)ã€‚
"""

def main():
    print(f"ğŸš€ ä½¿ç”¨åŸç”Ÿ Transformers åŠ è½½æ¨¡å‹ (BF16)...")
    
    # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œé˜²æ­¢è”ç½‘å¡æ­»
    os.environ["HF_HUB_OFFLINE"] = "1"
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    # 4090 æ˜¾å­˜å¤§ï¼Œç›´æ¥ç”¨ bfloat16 åŠ è½½ï¼Œæ—¢å¿«åˆç¨³
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        all_chunks = [json.loads(line) for line in f]
    
    dataset = []
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–æ—§æ•°æ®ç»§ç»­è·‘
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            print(f"ğŸ“‚ ç»§æ‰¿å·²æœ‰æ•°æ®: {len(dataset)} æ¡")
        except:
            pass

    print(f"ğŸ¯ å¼€å§‹æŒ–æ˜ (ç›®æ ‡: {TARGET_COUNT} æ¡)...")
    
    pbar = tqdm(total=len(all_chunks), desc="æ‰«æè¿›åº¦")
    
    for item in all_chunks:
        if len(dataset) >= TARGET_COUNT:
            break
            
        text = item['text']
        pbar.update(1)

        # === å¿«é€Ÿè¿‡æ»¤ (ä¸è´¹æ˜¾å¡) ===
        if "â€œ" not in text: continue # æ²¡å¯¹è¯ï¼Œè·³è¿‡
        if not any(r in text for r in TARGET_ROLES): continue # æ²¡ç†Ÿäººï¼Œè·³è¿‡
        if "æ— å¿Œ" not in text and "æ•™ä¸»" not in text: continue # æ²¡ä¸»è§’ï¼Œè·³è¿‡

        # === LLM æå– ===
        prompt = build_extraction_prompt(text)
        messages = [{"role": "user", "content": prompt}]
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text_input], return_tensors="pt").to(model.device)
        
        try:
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            response = response.replace("", "").strip()
            # ç®€å•ä¿®å¤ JSON å°¾éƒ¨
            if not response.endswith("]"):
                 idx = response.rfind("]")
                 if idx != -1: response = response[:idx+1]

            extracted = json.loads(response)
            
            if isinstance(extracted, list):
                saved_count = 0
                for pair in extracted:
                    if len(pair.get('output', '')) < 2: continue
                    
                    dataset.append({
                        "instruction": pair['instruction'],
                        "input": "",
                        "output": pair['output'],
                        "system": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œï¼Œè¯·ä»¥æ˜æ•™æ•™ä¸»çš„èº«ä»½ï¼Œç”¨æ­¦ä¾ é£æ ¼å›ç­”ã€‚"
                    })
                    saved_count += 1
                
                # å®æ—¶ä¿å­˜
                if saved_count > 0:
                    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                        json.dump(dataset, f, ensure_ascii=False, indent=2)
                    pbar.set_postfix({"å·²æå–": len(dataset)})
                    
        except:
            continue

    pbar.close()
    print(f"\nğŸ‰ å®Œæˆï¼å…±æå– {len(dataset)} æ¡æ•°æ®ã€‚")
    print(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()

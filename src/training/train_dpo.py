import os
import sys

# =====================================================
# ğŸ”¥ ä¿æŒç¯å¢ƒé”å®š
# =====================================================
print("ğŸ”§ ç¯å¢ƒæ£€æŸ¥ (transformers==4.41.2, trl==0.8.6)...")
# è¿™é‡Œå‡è®¾ä½ åˆšæ‰å·²ç»è¿è¡Œè¿‡å®‰è£…å‘½ä»¤äº†ï¼Œä¸éœ€è¦é‡å¤è·‘
# os.system("pip install transformers==4.41.2 trl==0.8.6 accelerate==0.30.1 -q")

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel
from trl import DPOTrainer

# ================= é…ç½®åŒº =================
MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
ADAPTER_PATH = "models/lora/zhangwuji_v4_family_fix" 
DATA_PATH = "data/dpo/train_dpo.json"
OUTPUT_DIR = "models/lora/zhangwuji_v5_dpo"
os.environ["HF_HUB_OFFLINE"] = "1"
# =========================================

def main():
    print("ğŸš€ [DPO] Loading Base Model...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH, quantization_config=bnb_config, device_map="auto", trust_remote_code=True
    )
    model = prepare_model_for_kbit_training(model)
    
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤åŒºï¼šå¼ºè¡Œç„Šæ­» Token IDï¼Œé˜²æ­¢ NoneType æŠ¥é”™ ğŸ”¥ğŸ”¥ğŸ”¥
    # Qwen çš„ <|im_end|> ID æ˜¯ 151645
    print("ğŸ”§ å¼ºåˆ¶è®¾ç½® Token IDs...")
    tokenizer.pad_token_id = 151645
    tokenizer.eos_token_id = 151645
    # DPO ç»å¸¸éœ€è¦ BOS tokenï¼ŒQwen æ²¡æœ‰ï¼Œæˆ‘ä»¬å¼ºè¡ŒæŒ‡å‘ EOSï¼Œé˜²æ­¢æŠ¥é”™
    tokenizer.bos_token_id = 151645 
    
    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼šåŒæ­¥ç»™æ¨¡å‹é…ç½®ï¼Œå¦åˆ™è®­ç»ƒå™¨å¯èƒ½è¯»å–é”™è¯¯çš„ config
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.bos_token_id = tokenizer.bos_token_id
    model.config.eos_token_id = tokenizer.eos_token_id

    # 2. åŠ è½½ SFT LoRA
    print(f"ğŸ”— Loading Adapter: {ADAPTER_PATH}")
    model = PeftModel.from_pretrained(model, ADAPTER_PATH, is_trainable=True)

    # 3. æ•°æ®å¤„ç†
    dataset = load_dataset("json", data_files=DATA_PATH, split="train")

    def process_dpo_data(example):
        messages = [
            {"role": "system", "content": example['system']},
            {"role": "user", "content": example['instruction']}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        return {
            "prompt": prompt,
            "chosen": example['chosen'],
            "rejected": example['rejected']
        }

    print("ğŸ”„ Formatting Data...")
    dataset = dataset.map(process_dpo_data)

    print("âš”ï¸ é…ç½® DPO Trainer...")
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=1e-6,
        max_steps=50,
        logging_steps=1,
        save_strategy="steps",
        save_steps=25,
        fp16=False,
        bf16=True,
        optim="paged_adamw_32bit",
        report_to="none",
        remove_unused_columns=False,
        # ğŸ”¥ æ˜¾å¼å‘Šè¯‰ Trainer å¿½ç•¥æŸäº›ä¸åŒ¹é…ï¼Œé˜²æ­¢å®ƒè‡ªä½œèªæ˜å»æ£€æŸ¥
        label_names=["labels"] 
    )

    trainer = DPOTrainer(
        model=model,
        ref_model=None,
        tokenizer=tokenizer,
        train_dataset=dataset,
        args=training_args,
        beta=0.1, 
        max_length=1024,
        max_prompt_length=512,
    )

    print("ğŸ”¥ Starting DPO Training...")
    trainer.train()

    print(f"ğŸ’¾ Saving DPO Model to: {OUTPUT_DIR}")
    trainer.model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("âœ… DPO è®­ç»ƒå¤§æˆåŠŸï¼")

if __name__ == "__main__":
    main()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import datetime
import re

# ================= é…ç½®åŒº =================
BASE_MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = "models/lora/zhangwuji_v1_fixed"
LOG_FILE = "logs/chat_history_final.jsonl"
SYSTEM_PROMPT = "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚è¯·ä»¥æ˜æ•™æ•™ä¸»çš„èº«ä»½ï¼Œç”¨ç®€ç»ƒã€ä¾ ä¹‰çš„å£å»å›ç­”ã€‚"
# =========================================

def smart_truncate(text):
    """
    âœ‚ï¸ æ™ºèƒ½å‰ªåˆ€ï¼šåªä¿ç•™ç²¾åï¼Œåˆ‡é™¤åºŸè¯
    """
    # 1. å¦‚æœåŒ…å«â€œUser:â€æˆ–â€œInstruction:â€ï¼Œè¯´æ˜æ¨¡å‹å¼€å§‹è‡ªè¨€è‡ªè¯­äº†ï¼Œç›´æ¥åˆ‡æ–­
    if "User:" in text: text = text.split("User:")[0]
    if "Instruction:" in text: text = text.split("Instruction:")[0]
    
    # 2. æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†å¥å­
    # åŒ¹é… ã€‚ï¼ï¼Ÿ... 
    sentences = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n)', text)
    
    # é‡æ–°ç»„åˆï¼Œåªä¿ç•™å‰ 3 ä¸ªå®Œæ•´å¥å­
    # (sentences åˆ—è¡¨é‡Œæ˜¯ [å¥1, æ ‡ç‚¹1, å¥2, æ ‡ç‚¹2, ...])
    keep_count = 6 # 3å¥è¯ * 2 (å†…å®¹+æ ‡ç‚¹)
    
    if len(sentences) > keep_count:
        # çœ‹çœ‹åé¢æ˜¯ä¸æ˜¯åºŸè¯ï¼Œå¦‚æœæ˜¯ï¼Œå°±åªå–å‰å‡ å¥
        truncated = "".join(sentences[:keep_count])
        return truncated
    else:
        return text

def main():
    print(f"ğŸš€ Loading Base Model: {BASE_MODEL_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"ğŸ”— Loading LoRA Adapter: {LORA_PATH}...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    print("="*50)
    print("ğŸ‰ å¼ æ— å¿Œï¼ˆç²¾ä¿®ç‰ˆï¼‰å·²ä¸Šçº¿ï¼")
    print("="*50)
    
    history = []
    
    while True:
        user_input = input("\nğŸ‘¤ ä½  (User): ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break
        if not user_input:
            continue
            
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            *history,
            {"role": "user", "content": user_input}
        ]
        
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|im_end|>")]
        
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=256,
                temperature=0.6, # æ¸©åº¦å†ä½ä¸€ç‚¹ï¼Œæ›´ç¨³
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=True,
                eos_token_id=stop_token_ids
            )
        
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        raw_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ è°ƒç”¨æ™ºèƒ½å‰ªåˆ€ ğŸ”¥ğŸ”¥ğŸ”¥
        final_response = smart_truncate(raw_response)
        
        print(f"ğŸ¤– å¼ æ— å¿Œ: {final_response}")
        
        if len(history) > 6: history = history[-6:]
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": final_response}) # å­˜å…¥å†å²çš„æ˜¯å‰ªåˆ‡åçš„å¹²å‡€ç‰ˆæœ¬
        
        log_entry = {"input": user_input, "output": final_response, "raw": raw_response}
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    main()
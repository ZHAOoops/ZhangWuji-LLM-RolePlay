import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import datetime

# ================= é…ç½®åŒº =================
BASE_MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = "models/lora/zhangwuji_v1_native"
LOG_FILE = "logs/chat_history.jsonl"

SYSTEM_PROMPT = "ä½ çŽ°åœ¨æ˜¯å¼ æ— å¿Œï¼Œèº«å¤„å€šå¤©å± é¾™è®°çš„æ­¦ä¾ ä¸–ç•Œã€‚è¯·ä»¥æ˜Žæ•™æ•™ä¸»çš„èº«ä»½ï¼Œç”¨è°¦å’Œã€ä¾ ä¹‰ä½†å¶å°”çº ç»“çš„å£å»å›žç­”ã€‚å›žç­”è¦ç®€çŸ­æœ‰åŠ›ï¼Œä¸è¦é•¿ç¯‡å¤§è®ºã€‚"
# =========================================

def main():
    print(f"ðŸš€ Loading Base Model: {BASE_MODEL_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"ðŸ”— Loading LoRA Adapter: {LORA_PATH}...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    print("="*50)
    print("ðŸŽ‰ å¼ æ— å¿Œå·²ä¸Šçº¿ï¼(è¾“å…¥ 'exit' é€€å‡º)")
    print("="*50)
    
    history = []
    
    while True:
        user_input = input("\nðŸ‘¤ ä½  (User): ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break
        if not user_input:
            continue
            
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            *history,
            {"role": "user", "content": user_input}
        ]
        
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=256, # ðŸ”¥ ç¼©çŸ­æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œå¼ºåˆ¶å®ƒç²¾ç®€
                temperature=0.6,    # ðŸ”¥ç¨å¾®é™ä½Žæ¸©åº¦ï¼Œå‡å°‘èƒ¡è¨€ä¹±è¯­
                top_p=0.9,
                repetition_penalty=1.2, # ðŸ”¥ æ ¸å¿ƒæ•‘å‘½è¯ï¼šå¦‚æžœå®ƒå¼€å§‹å¤è¯»ï¼Œç«‹åˆ»é‡ç½šï¼Œå¼ºè¡Œæ‰“æ–­
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id # ç¡®ä¿å®ƒçŸ¥é“æ€Žä¹ˆç»“æŸ
            )
        
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"ðŸ¤– å¼ æ— å¿Œ: {response}")
        
        # ç®€å•çš„åŽ†å²è®°å½•ç®¡ç†ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´å¤è¯»
        if len(history) > 10: 
            history = history[-10:]
            
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": response})
        
        log_entry = {
            "time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "input": user_input,
            "output": response
        }
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    main()
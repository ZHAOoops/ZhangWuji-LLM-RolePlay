import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import re
import argparse
import readline  # ğŸ”¥ å…³é”®ä¿®å¤ 1ï¼šå¼•å…¥ readlineï¼Œè®© input() æ”¯æŒé€€æ ¼å’Œå†å²

# ================= å‘½ä»¤è¡Œå‚æ•° =================
parser = argparse.ArgumentParser()
parser.add_argument("--version", type=str, default="v5_dpo", help="é»˜è®¤åŠ è½½ v5_dpoï¼Œä¹Ÿå¯ä»¥åˆ‡å› v4_family_fix")
args = parser.parse_args()

BASE_MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = f"models/lora/zhangwuji_{args.version}"
LOG_FILE = f"logs/chat_history_{args.version}.jsonl"

SYSTEM_PROMPT = """ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚
èº«ä»½ï¼šæ˜æ•™ç¬¬ä¸‰åå››ä»£æ•™ä¸»ï¼Œæ­¦å½“å¼ ç¿ å±±ä¸å¤©é¹°æ•™æ®·ç´ ç´ ä¹‹å­ï¼Œè°¢é€Šçš„ä¹‰å­ã€‚
æ€§æ ¼ï¼šå®½åšã€ä¾ ä¹‰ã€é‡æƒ…é‡ä¹‰ã€‚
å…³ç³»ï¼šèµµæ•æ˜¯çˆ±äººï¼ˆæœå»·éƒ¡ä¸»ï¼‰ï¼Œå‘¨èŠ·è‹¥æ˜¯é’æ¢…ç«¹é©¬ï¼ˆå³¨åµ‹æŒé—¨ï¼‰ï¼Œå¼ ä¸‰ä¸°æ˜¯å¤ªå¸ˆçˆ¶ã€‚
è¯­è¨€é£æ ¼ï¼šç®€ç»ƒã€å¤é£ã€‚"""

def clean_input_text(text):
    """ğŸ”¥ å…³é”®ä¿®å¤ 2ï¼šæ¸…æ´—è¾“å…¥ï¼Œå»æ‰ä¸å¯è§å­—ç¬¦å’Œæ§åˆ¶ç """
    if not text: return ""
    # å»æ‰åƒ \x08 (Backspace) è¿™æ ·çš„æ§åˆ¶å­—ç¬¦
    # åªä¿ç•™å¯æ‰“å°å­—ç¬¦ï¼Œæˆ–è€…æ±‰å­—
    cleaned = "".join(ch for ch in text if ch.isprintable() or '\u4e00' <= ch <= '\u9fff')
    return cleaned.strip()

def smart_truncate(text):
    if not text: return "ï¼ˆå¼ æ— å¿Œæ­£åœ¨æ²‰æ€â€¦â€¦ï¼‰"
    text = text.replace("User:", "").replace("Instruction:", "").strip()
    sentences = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n)', text)
    keep_count = 6 
    if len(sentences) > keep_count:
        return "".join(sentences[:keep_count])
    else:
        return text

def main():
    if not os.path.exists(LORA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {LORA_PATH}")
        return

    print(f"ğŸš€ Loading Base Model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"ğŸ”— Loading LoRA: {args.version} ...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    print("="*50)
    print(f"ğŸ‰ å¼ æ— å¿Œ [{args.version}] ç»ˆæä½“éªŒç‰ˆå·²ä¸Šçº¿ï¼")
    print("ğŸ’¡ æç¤ºï¼šç°åœ¨å¯ä»¥æ”¾å¿ƒä½¿ç”¨é€€æ ¼é”®ä¿®æ”¹é”™è¯¯äº†ã€‚")
    print("="*50)
    
    history = []
    
    while True:
        try:
            # åŸç”Ÿ input åœ¨ import readline åä¼šè‡ªåŠ¨å˜å¼º
            raw_input = input("\nğŸ‘¤ ä½  (User): ")
            
            # å†æ¬¡æ¸…æ´—ï¼ŒåŒé‡ä¿é™©
            user_input = clean_input_text(raw_input)
            
            if user_input.lower() in ["exit", "quit"]: break
            if not user_input: continue
            
            messages = [{"role": "system", "content": SYSTEM_PROMPT}, *history, {"role": "user", "content": user_input}]
            
            # è½¬ Tensor
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
            stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|im_end|>")]
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=256,
                    temperature=0.6,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=True,
                    eos_token_id=stop_token_ids
                )
            
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            raw_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            final_response = smart_truncate(raw_response)
            
            print(f"ğŸ¤– å¼ æ— å¿Œ: {final_response}")
            
            if len(history) > 8: history = history[-8:]
            history.append({"role": "user", "content": str(user_input)})
            history.append({"role": "assistant", "content": str(final_response)})
            
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps({"input": user_input, "output": final_response}, ensure_ascii=False) + "\n")
                
        except Exception as e:
            print(f"\nâŒ å¼‚å¸¸: {e}")
            if history: history.pop()

if __name__ == "__main__":
    main()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import datetime
import re

# ================= é…ç½®åŒº =================
BASE_MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = "models/lora/zhangwuji_v2_mixed"
LOG_FILE = "logs/chat_history_final.jsonl"

# ğŸ”¥ å‡çº§ç‰ˆ System Promptï¼šé”æ­»æŠ€èƒ½æ ‘ï¼Œé˜²æ­¢å¹»è§‰
SYSTEM_PROMPT = """ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚
èº«ä»½ï¼šæ˜æ•™ç¬¬ä¸‰åå››ä»£æ•™ä¸»ã€‚
æ€§æ ¼ï¼šå®½åšã€ä¾ ä¹‰ã€å¶å°”ä¼˜æŸ”å¯¡æ–­ï¼Œå¯¹é•¿è¾ˆæ­æ•¬ï¼Œå¯¹å¥³å­å¿ƒè½¯ã€‚
æ­¦åŠŸï¼šä¹é˜³ç¥åŠŸã€ä¹¾å¤å¤§æŒªç§»ã€å¤ªææ‹³å‰‘ã€åœ£ç«ä»¤æ­¦åŠŸã€‚ï¼ˆä¸¥ç¦èƒ¡ç¼–é™é¾™åå…«æŒã€å…­è„‰ç¥å‰‘ç­‰ä»–äººæ­¦åŠŸï¼‰
è¯­è¨€é£æ ¼ï¼šç®€ç»ƒã€å¤é£ã€‚é¢å¯¹ç°ä»£æ¦‚å¿µï¼ˆå¦‚æ‰‹æœºã€AIã€è‚¡å¸‚ï¼‰è¦è¡¨ç°å‡ºå¥½å¥‡ï¼Œæˆ–å°è¯•ç”¨æ±Ÿæ¹–é€»è¾‘å»ç†è§£ï¼Œä¸è¦æƒŠæã€‚"""
# =========================================

def smart_truncate(text):
    """æ™ºèƒ½å‰ªåˆ€"""
    if not text: return "ï¼ˆå¼ æ— å¿Œé™·å…¥æ²‰æ€â€¦â€¦ï¼‰"
    
    # æ¸…æ´—ç‰¹æ®Šæ ‡è®°
    text = text.replace("User:", "").replace("Instruction:", "")
    
    sentences = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n)', text)
    keep_count = 6 
    if len(sentences) > keep_count:
        return "".join(sentences[:keep_count])
    else:
        return text

def main():
    print(f"ğŸš€ Loading Base Model: {BASE_MODEL_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"ğŸ”— Loading LoRA Adapter: {LORA_PATH}...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    print("="*50)
    print("ğŸ‰ å¼ æ— å¿Œ V2 (é˜²çˆ†ç‰ˆ) å·²ä¸Šçº¿ï¼")
    print("="*50)
    
    history = []
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½  (User): ").strip()
            if user_input.lower() in ["exit", "quit"]:
                break
            if not user_input:
                continue
                
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                *history,
                {"role": "user", "content": user_input}
            ]
            
            # ğŸ”¥ é˜²çˆ†å¤„ç† 1ï¼šç¡®ä¿ text æ˜¯å­—ç¬¦ä¸²
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            if not text:
                print("âš ï¸ è­¦å‘Šï¼šæ¨¡æ¿ç”Ÿæˆä¸ºç©ºï¼Œè·³è¿‡æ­¤è½®")
                continue
                
            model_inputs = tokenizer([str(text)], return_tensors="pt").to(model.device)
            
            stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|im_end|>")]
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=256,
                    temperature=0.6,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=True,
                    eos_token_id=stop_token_ids
                )
            
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            raw_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            final_response = smart_truncate(raw_response)
            
            print(f"ğŸ¤– å¼ æ— å¿Œ: {final_response}")
            
            # ğŸ”¥ é˜²çˆ†å¤„ç† 2ï¼šé™åˆ¶å†å²é•¿åº¦ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡ºå¯¼è‡´å´©å
            if len(history) > 8: history = history[-8:]
            history.append({"role": "user", "content": user_input})
            history.append({"role": "assistant", "content": final_response})
            
            log_entry = {"input": user_input, "output": final_response}
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("   (å¼ æ— å¿Œè¿™ä¼šå„¿å¯èƒ½èµ°ç«å…¥é­”äº†ï¼Œè¯·æ¢ä¸ªè¯é¢˜é‡è¯•)")
            # å‡ºé”™æ—¶æ¸…ç©ºæœ€è¿‘ä¸€æ¡å†å²ï¼Œé˜²æ­¢æ­»å¾ªç¯
            if history: history.pop()

if __name__ == "__main__":
    main()

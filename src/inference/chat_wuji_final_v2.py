import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import re
import argparse

# ================= å‘½ä»¤è¡Œå‚æ•° =================
parser = argparse.ArgumentParser()
parser.add_argument("--version", type=str, default="v3_final", help="é»˜è®¤åŠ è½½ v3_final")
args = parser.parse_args()

BASE_MODEL_PATH = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = f"models/lora/zhangwuji_{args.version}"
LOG_FILE = f"logs/chat_history_{args.version}.jsonl"

# ğŸ”¥ System Prompt å†æ¬¡åŠ å¼ºï¼šæŠŠäº²å±å…³ç³»å†™æ­»åœ¨è¿™é‡Œï¼Œä½œä¸ºæœ€åä¸€é“é˜²çº¿
SYSTEM_PROMPT = """ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚
èº«ä»½ï¼šæ˜æ•™ç¬¬ä¸‰åå››ä»£æ•™ä¸»ï¼Œæ­¦å½“å¼ ç¿ å±±ä¸å¤©é¹°æ•™æ®·ç´ ç´ ä¹‹å­ï¼Œè°¢é€Šçš„ä¹‰å­ã€‚
æ€§æ ¼ï¼šå®½åšã€ä¾ ä¹‰ã€å¶å°”ä¼˜æŸ”å¯¡æ–­ã€‚
å…³ç³»ï¼šèµµæ•æ˜¯çˆ±äººï¼ˆæœå»·éƒ¡ä¸»ï¼‰ï¼Œå‘¨èŠ·è‹¥æ˜¯é’æ¢…ç«¹é©¬ï¼ˆå³¨åµ‹æŒé—¨ï¼‰ï¼Œæ®·ç¦»æ˜¯è¡¨å¦¹ã€‚
è¯­è¨€é£æ ¼ï¼šç®€ç»ƒã€å¤é£ã€‚é¢å¯¹ç°ä»£æ¦‚å¿µè¦è¡¨ç°å‡ºå¥½å¥‡ã€‚"""

def smart_truncate(text):
    if not text: return "ï¼ˆå¼ æ— å¿Œæ­£åœ¨æ²‰æ€â€¦â€¦ï¼‰"
    text = text.replace("User:", "").replace("Instruction:", "").strip()
    sentences = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n)', text)
    keep_count = 6 
    if len(sentences) > keep_count:
        return "".join(sentences[:keep_count])
    else:
        return text

def main():
    if not os.path.exists(LORA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {LORA_PATH}")
        return

    print(f"ğŸš€ Loading Base Model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"ğŸ”— Loading LoRA: {args.version} ...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    print("="*50)
    print(f"ğŸ‰ å¼ æ— å¿Œ [{args.version}] ç¨³å®šé˜²çˆ†ç‰ˆå·²ä¸Šçº¿ï¼")
    print("="*50)
    
    history = []
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½  (User): ").strip()
            if user_input.lower() in ["exit", "quit"]: break
            if not user_input: continue
            
            messages = [{"role": "system", "content": SYSTEM_PROMPT}, *history, {"role": "user", "content": user_input}]
            
            # ğŸ”¥ ä¿®å¤æ ¸å¿ƒï¼šå…ˆè½¬å­—ç¬¦ä¸²ï¼Œæ‰“å° debugï¼Œå†è½¬ Tensor
            # è¿™æ ·ç»å¯¹ä¸ä¼šæŠ¥ TypeErrorï¼Œå› ä¸ºæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶äº†æµç¨‹
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            # æ‰“å°ä¸€ä¸‹çœ‹çœ‹æ¨¡å‹åˆ°åº•åƒäº†ä»€ä¹ˆï¼ˆè°ƒè¯•ç”¨ï¼Œç¨³å®šåå¯æ³¨é‡Šæ‰ï¼‰
            # print(f"\n[Debug] è¾“å…¥æ¨¡å‹çš„æ–‡æœ¬:\n{text[-100:]}...\n") 
            
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
            stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|im_end|>")]
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=256,
                    temperature=0.6,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=True,
                    eos_token_id=stop_token_ids
                )
            
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            raw_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            final_response = smart_truncate(raw_response)
            
            print(f"ğŸ¤– å¼ æ— å¿Œ: {final_response}")
            
            if len(history) > 8: history = history[-8:]
            history.append({"role": "user", "content": str(user_input)})
            history.append({"role": "assistant", "content": str(final_response)})
            
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps({"input": user_input, "output": final_response}, ensure_ascii=False) + "\n")
                
        except Exception as e:
            print(f"\nâŒ å¼‚å¸¸: {e}")
            # æ‰“å°å †æ ˆä»¥ä¾¿æ’æŸ¥
            import traceback
            traceback.print_exc()
            if history: history.pop()

if __name__ == "__main__":
    main()
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import os

# 1. è·¯å¾„é…ç½®
model_path = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
data_path = "data/test_set/exam.json"

print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨åŸç”Ÿ Transformers åŠ è½½æ¨¡å‹: {model_path} ...")

# 2. åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# 3. åŠ è½½æ¨¡å‹ (ä½¿ç”¨ bfloat16 åŸç”Ÿç²¾åº¦ï¼Œæ˜¾å­˜å ç”¨çº¦ 15GBï¼Œ4090 è½»æ¾æ‹¿ä¸‹)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¼€å§‹ç­”é¢˜...\n")

# 4. è¯»å–è¯•é¢˜
with open(data_path, "r", encoding="utf-8") as f:
    questions = json.load(f)

# 5. å¾ªç¯ç­”é¢˜
print("="*60)
for i, item in enumerate(questions):
    q = item['question']
    
    # æ„é€  Prompt
    messages = [
        {"role": "system", "content": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚"},
        {"role": "user", "content": q}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # ç”Ÿæˆå›ç­”
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
        temperature=0.7,   # ç¨å¾®æœ‰ç‚¹åˆ›é€ åŠ›
        top_p=0.9
    )
    
    # è§£ç 
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # æ‰“å°
    print(f"ğŸ“ é—®é¢˜ [{i+1}]: {q}")
    print(f"ğŸ—£ï¸ å›ç­”: {response}")
    print("-" * 60)

print("ğŸ‰ å…¨éƒ¨æµ‹è¯•å®Œæˆï¼")

from unsloth import FastLanguageModel
import torch
import json
import os

# å¼ºåˆ¶ç¦»çº¿ï¼Œé˜²æ­¢å®ƒå»è”ç½‘æŸ¥æ›´æ–°
os.environ["HF_HUB_OFFLINE"] = "1"

# 1. åŠ è½½æˆ‘ä»¬å·²ç»ä¸‹è½½å¥½çš„åŸºåº§æ¨¡å‹
model_path = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
print(f"ğŸš€ Loading Qwen2.5 from: {model_path} ...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_path,
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

# 2. åŠ è½½è¯•é¢˜
with open("data/test_set/exam.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

print("\n" + "="*50)
print("ğŸ¤– å¼ æ— å¿Œ (Base Model) å¼€å§‹ç­”é¢˜...")
print("="*50 + "\n")

# 3. å¾ªç¯åšé¢˜
for i, item in enumerate(questions):
    q = item['question']
    
    # æ„é€  Prompt
    messages = [
        {"role": "system", "content": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚"},
        {"role": "user", "content": q}
    ]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    # ç”Ÿæˆ
    outputs = model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.7)
    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)

    # æ‰“å°ç»“æœ
    print(f"ğŸ“ é¢˜ç›® [{i+1}]: {q}")
    print(f"ğŸ—£ï¸ å›ç­”: {response}")
    print("-" * 50)

print("âœ… æ‰€æœ‰é¢˜ç›®å›ç­”å®Œæ¯•ï¼")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import pandas as pd
import os
from datetime import datetime

# 1. é…ç½®
model_path = "/root/autodl-tmp/ZhangWuji_Project/models/base/Qwen/Qwen2.5-7B-Instruct"
data_path = "data/test_set/exam.json"
output_dir = "logs/eval_reports"
os.makedirs(output_dir, exist_ok=True)

print(f"ğŸš€ Loading Model from: {model_path} ...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    device_map="auto", 
    torch_dtype=torch.bfloat16, 
    trust_remote_code=True
)

# 2. è¯»å–é¢˜ç›®
with open(data_path, "r", encoding="utf-8") as f:
    questions = json.load(f)

results = []
print("="*60)

# 3. ç­”é¢˜å¹¶è®°å½•
for item in questions:
    q = item['question']
    ref = item['ref_answer']
    
    messages = [
        {"role": "system", "content": "ä½ ç°åœ¨æ˜¯å¼ æ— å¿Œã€‚"},
        {"role": "user", "content": q}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9
    )
    
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    # å­˜å…¥åˆ—è¡¨
    results.append({
        "question": q,
        "ref_answer": ref,
        "base_model_answer": response
    })
    print(f"âœ… å·²è®°å½•: {q[:10]}...")

# 4. ä¿å­˜ä¸º CSV
df = pd.DataFrame(results)
timestamp = datetime.now().strftime("%Y%m%d")
filename = f"{output_dir}/exam_result_baseline_{timestamp}.csv"
df.to_csv(filename, index=False, encoding="utf-8-sig")

print("="*60)
print(f"ğŸ‰ å­˜æ¡£å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {filename}")
print("è¿™å°±æ˜¯æˆ‘ä»¬çš„â€˜ä¸€å·é€‰æ‰‹â€™ç­”å·ï¼Œè¯·åŠ¡å¿…å¦¥å–„ä¿ç®¡ï¼")
